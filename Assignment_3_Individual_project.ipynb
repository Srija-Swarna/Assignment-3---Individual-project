{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPcDrUBTpvQHDvNStXmo+zP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srija-Swarna/Assignment-3---Individual-project/blob/main/Assignment_3_Individual_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code starts by importing a set of necessary libraries and modules. numpy is used for numerical operations, datasets from the Hugging Face library is utilized to load and handle datasets, and transformers provides tools for using BERT (Bidirectional Encoder Representations from Transformers), a powerful transformer-based model. Specifically, BertTokenizer and BertForSequenceClassification are used for tokenizing text and for defining the BERT model with a sequence classification task. Trainer and TrainingArguments are utilities for training the model, while DataCollatorWithPadding helps in dynamically padding the input data. The pipeline function is used for easy application of the model to new inputs. For evaluation, accuracy_score from sklearn.metrics is used to calculate accuracy"
      ],
      "metadata": {
        "id": "WG-SZeTgWVSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlvnaZ4bTH-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683c73ec-1d87-4796-ab91-2932c5269811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Necessary Libraries\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "KhmfKCdgWyGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset \"dvilasuero/ag_news_error_analysis\" is loaded using the load_dataset function. This dataset contains text inputs and their associated annotations, which the model will use for training and evaluation. The structure and content of the dataset are printed to the console, allowing for an initial inspection of the data."
      ],
      "metadata": {
        "id": "uK3DaS_gXCtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading and Printing the Dataset\n",
        "dataset = load_dataset(\"dvilasuero/ag_news_error_analysis\")\n",
        "print(\"Dataset:\", dataset)"
      ],
      "metadata": {
        "id": "is73jDLcXbu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BERT tokenizer and model are loaded from the pre-trained 'bert-base-uncased' model. The tokenizer is responsible for converting raw text into token IDs that the model can process. The model is initialized for a sequence classification task with four labels, indicating it will classify inputs into one of four possible categories."
      ],
      "metadata": {
        "id": "jH6LVv_1ZQBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Tokenizer and the Model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)"
      ],
      "metadata": {
        "id": "Wl7qSmBCZSMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function preprocess_function is defined to tokenize the inputs from the dataset. This function uses the BERT tokenizer to encode the text data, ensuring each input is padded and truncated to a maximum length of 256 tokens. The function also sets the 'labels' field, which corresponds to the annotations in the dataset, making sure the labels are available during training. The dataset is then tokenized and converted into a format suitable for PyTorch tensors, with the necessary columns (input_ids, attention_mask, labels) retained."
      ],
      "metadata": {
        "id": "0-YPlLjNZcjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the Dataset\n",
        "def preprocess_function(examples):\n",
        "    tokenized = tokenizer(examples['inputs'], padding=\"max_length\", truncation=True, max_length=256)\n",
        "    tokenized['labels'] = examples['annotation']  # Ensure 'labels' are included\n",
        "    return tokenized\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "id": "uazitZhAZgcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training portion of the tokenized dataset is further split into a smaller training set and a validation set using the train_test_split method. This step is crucial for evaluating the model's performance on unseen data (validation set) during training, helping to monitor for overfitting and to tune hyperparameters."
      ],
      "metadata": {
        "id": "53gKFDB4aD5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the Training dataset into Train and Validation sets\n",
        "small_train_dataset = tokenized_datasets['train'].train_test_split(test_size=0.1)\n",
        "train_dataset = small_train_dataset['train']\n",
        "eval_dataset = small_train_dataset['test']"
      ],
      "metadata": {
        "id": "O2BdHc0NaanF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A DataCollatorWithPadding is instantiated, which pads the inputs dynamically to the same length within a batch. This is essential for efficient processing in batches during training. The TrainingArguments object is configured with various parameters: the directory for saving model outputs (output_dir), evaluation strategy (epoch to evaluate after each epoch), learning rate, batch sizes for training and evaluation, number of training epochs, weight decay for regularization, and logging settings."
      ],
      "metadata": {
        "id": "PCGwPfKFaszx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the Data Collator and Training Arguments\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")"
      ],
      "metadata": {
        "id": "_LILajXca2U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The compute_metrics function calculates the accuracy of the model's predictions. It takes the output of the model predictions and computes the predicted class labels by taking the argmax of the logits. It then compares these predictions to the true labels (p.label_ids) to calculate the accuracy score using accuracy_score from scikit-learn."
      ],
      "metadata": {
        "id": "xhfc1zA9eZ9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the compute_metrics Function\n",
        "def compute_metrics(p):\n",
        "    predictions = np.argmax(p.predictions, axis=1)\n",
        "    return {\"accuracy\": accuracy_score(p.label_ids, predictions)}"
      ],
      "metadata": {
        "id": "enNs6l-we5LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Trainer class is instantiated with the model, training arguments, training dataset, evaluation dataset, data collator, and the metrics computation function. The Trainer class simplifies the training loop by managing the training process, including model updates, evaluation, and logging."
      ],
      "metadata": {
        "id": "DWmNt6WBe-s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # Provide the evaluation dataset\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "NelaPdx9dZ-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training process is initiated using the trainer.train() method. During training, the model learns to minimize the loss by adjusting its weights based on the backpropagation of errors. The model's performance is periodically evaluated on the validation set, and the best model (if applicable) is saved to the specified output directory."
      ],
      "metadata": {
        "id": "VpbXpKTSgBYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "7Q-QIECjxJxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QuRPAz87-tcl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62xCefEBU6jK"
      },
      "source": [
        "After training, the model's performance is evaluated on the validation set using the trainer.evaluate() method. This provides metrics, including validation accuracy, which is printed to the console."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Validation Accuracy: {eval_results.get('eval_accuracy', 'N/A'):.4f}\")"
      ],
      "metadata": {
        "id": "b2Fje9cw-tET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions are made on the evaluation dataset, and the resulting predicted labels (y_pred) are compared with the true labels (y_true). A confusion matrix is computed and visualized using Seaborn to show the number of correct and incorrect predictions for each class. This visualization helps to understand the model's performance in detail. Additionally, a classification report is generated using classification_report from scikit-learn, providing precision, recall, F1-score, and support for each class. This report is printed to give a comprehensive view of the model's classification performance."
      ],
      "metadata": {
        "id": "4HburI0AG8aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting from the Evaluation Dataset\n",
        "predictions = trainer.predict(eval_dataset)"
      ],
      "metadata": {
        "id": "ueeNmjO_G-4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicted Labels and True Labels\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "y_true = predictions.label_ids"
      ],
      "metadata": {
        "id": "-5kDcJ_4X3uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)"
      ],
      "metadata": {
        "id": "nmZoivQyX_YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the Confusion Matrix Using Seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(4), yticklabels=range(4))\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0INIhIRRYIXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a Classification Report\n",
        "report = classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3'])\n",
        "print(report)"
      ],
      "metadata": {
        "id": "TimRZEqAv_J-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifying the specific sentences and comparing with true labels\n",
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "uj5W7k_dwGk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A text classification pipeline is created using the fine-tuned model and tokenizer. This pipeline allows easy application of the model to new sentences. The provided sentences are classified, and the predicted labels are output. These predicted labels are then compared with the true labels, which are either \"Sci-Tech Annotation\" or \"Business Annotation.\" The true and predicted labels for each sentence are printed, demonstrating the model's real-world application and effectiveness in categorizing new inputs."
      ],
      "metadata": {
        "id": "RVpeMPGw0HWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sentences\n",
        "sentences = [\n",
        "    {'text': 'The Politics of Time and Dispossession Make a commitment to subscribe, donate and/or place all of your book and other product orders from Amazon.com and others through MMN Shopping web-site by clicking here.'},\n",
        "    {'text': 'TDK launches 400GB data cartridge TDK will start selling new 400GB data storage cartridges based on the LTO (Linear Tape-Open) Ultrium 3 format in Japan and elsewhere from the middle of November.'},\n",
        "    {'text': 'With Charley on Their Minds, Floridians Brace for Round 2 Al Cialella is from Philadelphia, where he always thought of quot;sturdy quot; oak trees as symbols of strength. He never knew that in central Florida, the water table '},\n",
        "    {'text': 'Wealth in America In what has become an annual ritual, Forbes Magazine has released its \"400 Richest in America\", a list of the wealthiest in a land which is simultaneously revered and reviled for its wealth. Between the lines are some suprisingly revealing truths about what it takes to be rich in America.'},\n",
        "    {'text': 'Court Deals Blow to Anti-Piracy Efforts (Reuters) Reuters - A federal appeals court on Thursday\\\\delivered a stinging blow to the anti-piracy efforts of major\\\\movie studios and music companies, ruling that several major\\\\online file-sharing software companies are not liable for\\\\copyright infringement.'},\n",
        "    {'text': '2 Vietnamese die of unidentified virus Among three recent human deaths in Vietnam #39;s northern Thai Nguyen province, two were initially reported to have contracted an unidentified dangerous virus, local newspaper Labor reported Monday.'}\n",
        "]"
      ],
      "metadata": {
        "id": "l_8aMwBQ0M_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True Labels\n",
        "true_labels = [\n",
        "    'Business Annotation',\n",
        "    'Sci-Tech Annotation',\n",
        "    'Other',\n",
        "    'Business Annotation',\n",
        "    'Sci-Tech Annotation',\n",
        "    'Sci-Tech Annotation'\n",
        "]"
      ],
      "metadata": {
        "id": "YnqwvHxB4-QI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}